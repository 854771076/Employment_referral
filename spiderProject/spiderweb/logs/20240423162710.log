INFO - 2024-04-23 16:27:12,316 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 105 - scheduler - successfully synced task with jobs with force
ERROR - 2024-04-23 17:10:38,454 - process: 30560 - utils.py - gerapy.server.core.utils - 564 - utils - Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 264, in connect
    sock = self.retry.call_with_retry(
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\retry.py", line 46, in call_with_retry
    return do()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 265, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 627, in _connect
    raise err
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 615, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 38, in <module>
    main()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 34, in main
    execute()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\cmdline.py", line 160, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 357, in __init__
    super().__init__(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 227, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 221, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 79, in from_settings
    return cls(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 34, in __init__
    self._load_all_spiders()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 63, in _load_all_spiders
    for module in walk_modules(name):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\utils\misc.py", line 106, in walk_modules
    submod = import_module(fullpath)
  File "D:\conda\envs\jobfreeSpider\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 655, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 618, in _load_backward_compatible
  File "<frozen zipimport>", line 259, in load_module
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1710814321.egg\tutorial\spiders\zhilian.py", line 15, in <module>
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1710814321.egg\tutorial\spiders\zhilian.py", line 21, in TestSpider
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\commands\core.py", line 1829, in get
    return self.execute_command("GET", name)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\client.py", line 533, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 1086, in get_connection
    connection.connect()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 270, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to 127.0.0.1:6379. 由于目标计算机积极拒绝，无法连接。.
Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\gerapy\server\core\utils.py", line 562, in wrapper
    result = func(*args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\django\views\generic\base.py", line 71, in view
    return self.dispatch(request, *args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\gerapy\server\core\views.py", line 189, in spider_list
    spiders = scrapyd.list_spiders(project_name)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd_api\wrapper.py", line 158, in list_spiders
    json = self.client.get(url, params=params, timeout=self.timeout)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd_api\client.py", line 38, in request
    return self._handle_response(response)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd_api\client.py", line 34, in _handle_response
    raise ScrapydResponseError(json['message'])
scrapyd_api.exceptions.ScrapydResponseError: Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 264, in connect
    sock = self.retry.call_with_retry(
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\retry.py", line 46, in call_with_retry
    return do()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 265, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 627, in _connect
    raise err
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 615, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 38, in <module>
    main()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 34, in main
    execute()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\cmdline.py", line 160, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 357, in __init__
    super().__init__(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 227, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 221, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 79, in from_settings
    return cls(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 34, in __init__
    self._load_all_spiders()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 63, in _load_all_spiders
    for module in walk_modules(name):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\utils\misc.py", line 106, in walk_modules
    submod = import_module(fullpath)
  File "D:\conda\envs\jobfreeSpider\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 655, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 618, in _load_backward_compatible
  File "<frozen zipimport>", line 259, in load_module
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1710814321.egg\tutorial\spiders\zhilian.py", line 15, in <module>
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1710814321.egg\tutorial\spiders\zhilian.py", line 21, in TestSpider
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\commands\core.py", line 1829, in get
    return self.execute_command("GET", name)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\client.py", line 533, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 1086, in get_connection
    connection.connect()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 270, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to 127.0.0.1:6379. 由于目标计算机积极拒绝，无法连接。.

INFO - 2024-04-23 17:10:47,586 - process: 30560 - build.py - gerapy.server.core.build - 24 - build - successfully build project spider to egg file C:\Users\Administrator\Desktop\Employment_referral\spiderProject\spiderweb\projects\spider\spider-1.0-py3.8.egg
ERROR - 2024-04-23 17:10:51,367 - process: 30560 - utils.py - gerapy.server.core.utils - 564 - utils - Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 264, in connect
    sock = self.retry.call_with_retry(
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\retry.py", line 46, in call_with_retry
    return do()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 265, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 627, in _connect
    raise err
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 615, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 38, in <module>
    main()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 34, in main
    execute()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\cmdline.py", line 160, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 357, in __init__
    super().__init__(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 227, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 221, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 79, in from_settings
    return cls(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 34, in __init__
    self._load_all_spiders()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 63, in _load_all_spiders
    for module in walk_modules(name):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\utils\misc.py", line 106, in walk_modules
    submod = import_module(fullpath)
  File "D:\conda\envs\jobfreeSpider\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 655, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 618, in _load_backward_compatible
  File "<frozen zipimport>", line 259, in load_module
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1713863448.egg\tutorial\spiders\zhilian.py", line 15, in <module>
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1713863448.egg\tutorial\spiders\zhilian.py", line 21, in TestSpider
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\commands\core.py", line 1829, in get
    return self.execute_command("GET", name)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\client.py", line 533, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 1086, in get_connection
    connection.connect()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 270, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to 127.0.0.1:6379. 由于目标计算机积极拒绝，无法连接。.
Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\gerapy\server\core\utils.py", line 562, in wrapper
    result = func(*args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\django\views\generic\base.py", line 71, in view
    return self.dispatch(request, *args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\gerapy\server\core\views.py", line 461, in project_deploy
    scrapyd.add_version(project_name, int(time.time()), egg_file.read())
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd_api\wrapper.py", line 76, in add_version
    json = self.client.post(url, data=data, files=files,
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\requests\sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd_api\client.py", line 38, in request
    return self._handle_response(response)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd_api\client.py", line 34, in _handle_response
    raise ScrapydResponseError(json['message'])
scrapyd_api.exceptions.ScrapydResponseError: Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 264, in connect
    sock = self.retry.call_with_retry(
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\retry.py", line 46, in call_with_retry
    return do()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 265, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 627, in _connect
    raise err
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 615, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "D:\conda\envs\jobfreeSpider\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 38, in <module>
    main()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapyd\runner.py", line 34, in main
    execute()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\cmdline.py", line 160, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 357, in __init__
    super().__init__(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 227, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\crawler.py", line 221, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 79, in from_settings
    return cls(settings)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 34, in __init__
    self._load_all_spiders()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\spiderloader.py", line 63, in _load_all_spiders
    for module in walk_modules(name):
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\scrapy\utils\misc.py", line 106, in walk_modules
    submod = import_module(fullpath)
  File "D:\conda\envs\jobfreeSpider\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 655, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 618, in _load_backward_compatible
  File "<frozen zipimport>", line 259, in load_module
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1713863448.egg\tutorial\spiders\zhilian.py", line 15, in <module>
  File "c:\users\administrator\desktop\employment_referral\spiderproject\scrapyd\eggs\spider\1713863448.egg\tutorial\spiders\zhilian.py", line 21, in TestSpider
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\commands\core.py", line 1829, in get
    return self.execute_command("GET", name)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\client.py", line 533, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 1086, in get_connection
    connection.connect()
  File "D:\conda\envs\jobfreeSpider\lib\site-packages\redis\connection.py", line 270, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to 127.0.0.1:6379. 由于目标计算机积极拒绝，无法连接。.

INFO - 2024-04-24 00:00:00,012 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian
INFO - 2024-04-24 00:00:00,040 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian2
INFO - 2024-04-26 00:00:00,087 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian
INFO - 2024-04-26 00:00:00,104 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian2
INFO - 2024-04-27 00:00:00,063 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian
INFO - 2024-04-27 00:00:00,080 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian2
INFO - 2024-04-28 00:00:00,087 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian
INFO - 2024-04-28 00:00:00,106 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian2
INFO - 2024-04-29 00:00:00,075 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian
INFO - 2024-04-29 00:00:00,087 - process: 30560 - scheduler.py - gerapy.server.core.scheduler - 34 - scheduler - execute job of client JOB, project spider, spider zhilian2
